{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JoseQuintanaProyecto_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U9BlR4PTl8IE",
        "vRXqTgz1Qy-E",
        "MQ6V1hi5oY9j"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQG6wMHMut8GBNEGPQdqKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmquintana/data_science_sprint_2/blob/main/JoseQuintanaProyecto_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMTInXQMRN3y"
      },
      "source": [
        "# **Sprint Proyect 2**\n",
        "**Ingeniería de Features, Modelos Avanzados e Interpretación de Modelos (4 sprints)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYt2QIfnU6Z"
      },
      "source": [
        "## **Story Points**\n",
        "<img src='https://s3.amazonaws.com/platform-resources.acamica.com/slides/Im%C3%A1genes+en+Toolboxes/DS+(4+Sprints)/ds_sprintproject2_a_actuallizado.png' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MVNanq2tDya"
      },
      "source": [
        "### **Transformación de Datos**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeJ6MWXgowVU"
      },
      "source": [
        "#### **Carga del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGVacN5C4OM"
      },
      "source": [
        "# Importamos las librerías que vamos a necesitar\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "from google.colab import data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_6Dgeu1Xc3"
      },
      "source": [
        "# Le doy formato a los gráficos\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.rc('font', size=10)\n",
        "plt.rc('axes', titlesize=16)\n",
        "plt.rc('figure', titlesize=16)\n",
        "plt.rc('axes', labelsize=14) \n",
        "plt.rc('xtick', labelsize=12) \n",
        "plt.rc('ytick', labelsize=12) "
      ],
      "execution_count": 735,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y71Vw4X3XIjQ"
      },
      "source": [
        "WORK_WITH_SAMPLE = False # esto al inicio de todo"
      ],
      "execution_count": 736,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "h1bNT_of73MR",
        "outputId": "e11d756f-6fc7-43dd-bb8f-b03eaab512bb"
      },
      "source": [
        "#Seteamos para que no utilice notacion cientifica\n",
        "pd.options.display.float_format = '{:.4f}'.format\n",
        "#Seteo para que el máximo de columnas que muestra al levantar una base sean 500\n",
        "pd.set_option('display.max_columns',500)\n",
        "#Estos códigos hacen que la visualización de la consola abarque toda la pantalla (sin los recortes a los costados). Tambien hacen que al mostrar dataframes podamos ver todas las columnas que tiene.\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "# Codigo para poder imprimir multiples outputs en una misma línea\n",
        "# from IPython.core.interactiveshell import InteractiveShell\n",
        "# InteractiveShell.ast_node_interactivity = \"all\"\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": 737,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL2eAd_tC4Oa",
        "outputId": "cc2fcebd-88f5-438f-f838-7bc64d5b7fdd"
      },
      "source": [
        "# Monto mi Google Drive para cargar el DataSet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 738,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CauRgMsgHUgE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "dfb00529-401a-4d3b-d383-9db08232a076"
      },
      "source": [
        "# Cargo el DataSet con Pandas como un DataFrame nombrado \"df\"\n",
        "# Previamente debe descargarse del siguiente link: https://drive.google.com/uc?export=download&id=1Ugbsw5XbNRbglomSQO1qkAgMFB_3BzmB\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DS_Proyecto_01_Datos_Properati.csv\")\n",
        "# df = pd.read_csv(\"DS_Proyecto_01_Datos_Properati.csv\")"
      ],
      "execution_count": 739,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-739-9f90083f4341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cargo el DataSet con Pandas como un DataFrame nombrado \"df\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Previamente debe descargarse del siguiente link: https://drive.google.com/uc?export=download&id=1Ugbsw5XbNRbglomSQO1qkAgMFB_3BzmB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/DS_Proyecto_01_Datos_Properati.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# df = pd.read_csv(\"DS_Proyecto_01_Datos_Properati.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpPuZXHAXODj"
      },
      "source": [
        "if WORK_WITH_SAMPLE:\n",
        "  df = df.sample(n=10000, random_state=99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7S9JoAktqiw"
      },
      "source": [
        "print(\"Columnas:\",df.shape[1])\n",
        "print(\"Filas:\",df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz6y4MByuKjS"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa8y63YYuOED"
      },
      "source": [
        "# Elimino columnas que no necesitaré y renombro las columnas\n",
        "df = df[['lat', 'lon', 'l2', 'l3', 'property_type', 'rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered', 'price']]\n",
        "df = df.rename({'l2':'zona', 'l3':'barrio', 'rooms':'ambientes', 'bedrooms':'dormitorios', 'bathrooms':'baños', 'surface_total':'sup_total', 'surface_covered':'sup_cubierta', 'price':'precio', 'property_type':'tipo'}, axis=1)\n",
        "%load_ext google.colab.data_table\n",
        "data_table.DataTable(df, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMfdt4WOSWxD"
      },
      "source": [
        "# Filtro por las propiedades de la zona de Capital Federal\n",
        "%unload_ext google.colab.data_table\n",
        "df = df[df.zona == 'Capital Federal']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhqmdlvsTJB2"
      },
      "source": [
        "# Agrupo las propiedades por `tipo` para ver su participación dentro de la muestra\n",
        "df_group = df.groupby('tipo').count().precio / df.shape[0]\n",
        "df_group.sort_values(ascending=False, inplace=True)\n",
        "df_group = df_group.reset_index(name='rel')\n",
        "# df_group['rel'] = pd.Series([\"{0:.1f}%\".format(val * 100) for val in df_group['rel']])\n",
        "df_group"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx9QKf9lVlZx"
      },
      "source": [
        "# Descarto las instancias del dataset cuyo ´tipo´ tenga una articipación menor al 1% del total (ya que no considero representativa)\n",
        "# Me quedo con los tipos Departamento, Casa y PH\n",
        "df = df[df.tipo.isin(['Departamento', 'Casa', 'PH'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqad6U7_X-yz"
      },
      "source": [
        "A continuación realizamos algunas verificaciones extra: que no haya `sup_cubierta` > `sup_total`, que no haya instancias duplicadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAIcMEgjX-Nk"
      },
      "source": [
        "# Verifico que la superfice cubierta no sea mayor que la total\n",
        "# A continuación vamos a filtrar aquellas propiedades que posean 'surface_covered' > 'surface_total' ya que son  inconsistencias del dataset.\n",
        "mascara = (df.sup_cubierta) > (df.sup_total)\n",
        "print(\"La cantidad de instancias que tienen 'sup_cubierta' mayor que 'sup_total' es:\", df[mascara].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgbZduoBYg95"
      },
      "source": [
        "# Reasignamos esas instancias con el valor de `sup_total`\n",
        "df.loc[mascara, 'sup_cubierta'] = df.loc[mascara, 'sup_total']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJAtyILCcmBA"
      },
      "source": [
        "# Verificamos las instancias duplicadas\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEei-F51b1C7"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsJAdxlKRZVG"
      },
      "source": [
        "def hmap(df):\n",
        "  plt.figure(figsize=(12,10))\n",
        "  corr_matrix = df.corr()\n",
        "  mask = np.zeros_like(corr_matrix)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  sns.heatmap(corr_matrix, vmin=-0.5, annot=True, square=True, mask=mask, cmap='BrBG')\n",
        "  plt.show()\n",
        "  \n",
        "hmap(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmqvcEjytIvo"
      },
      "source": [
        "#### **Elección de transformaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUBhh00H09GB"
      },
      "source": [
        "##### 1. Encoding\n",
        "\n",
        "Vamos a realizar el encoding de las variables categóricas: ***zona***, ***barrio*** y ***tipo*** para poder utilizar éstos atributos como input del modelo predictor. Las 3 variables son de tipo nominal por lo que aplicaría ***One Hot Enconding***. Sin embargo, la columna ***barrio*** presenta muchas instancias distintas, por lo que realizar un One Hot Encoding de ella generaría muchas columnas adicionales y encarecería el procesamiento de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aE3o_VpO8Qw"
      },
      "source": [
        "##### 2. Análisis de Valores Faltantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_m4IRn8nSl"
      },
      "source": [
        "# Cantidad de valores faltantes por columna del dataset\n",
        "faltantes = pd.DataFrame(df.isnull().sum(), columns=['faltantes'])\n",
        "faltantes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx3smrptPln_"
      },
      "source": [
        "###### 2.1. Faltantes de variable `baños`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakDZzFX4QaL"
      },
      "source": [
        "# Análisis faltantes en variable 'baños'\n",
        "# Agrupo los Baños faltantes por Tipo de propiedad\n",
        "df_baños_faltantes = df.baños.isnull().groupby([df['tipo']]).sum().astype(int).reset_index(name='missings')\n",
        "df_baños_faltantes['total'] = df.precio.groupby(df['tipo']).count().values\n",
        "df_baños_faltantes['rel'] = df_baños_faltantes.missings / df_baños_faltantes.total\n",
        "df_baños_faltantes.sort_values(by='rel', ascending=False, inplace=True)\n",
        "df_baños_faltantes['%'] = pd.Series([\"{0:.2f}%\".format(val * 100) for val in df_baños_faltantes['rel']])\n",
        "df_baños_faltantes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW2pb6Hd1Hdx"
      },
      "source": [
        "# Análisis faltantes en variable 'baños'\n",
        "faltantes_baño = df[df.baños.isna()]\n",
        "tipos = faltantes_baño.tipo.value_counts()\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.title('Cantidad porcentual de faltantes de Baño por Tipo de propiedad')\n",
        "ax = sns.barplot(data=df_baños_faltantes, x='tipo', y='rel')\n",
        "plt.ylabel('Cantidad de faltantes (%)')\n",
        "plt.xlabel('Tipo de propiedad')\n",
        "plt.xticks(rotation=0)\n",
        "ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYnFZ6g3GQgf"
      },
      "source": [
        "Para solucionar el tema de los valores faltantes de la variable `baños` podría imputarlos con el valor de la moda para cada `tipo` de propiedad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajc5YWaKdDbj"
      },
      "source": [
        "sns.boxplot(data=df, y='tipo', x='baños', order=df_baños_faltantes.tipo)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckYgtlza24Bu"
      },
      "source": [
        "# La moda de la variable `baños` por cada tipo de propiedad\n",
        "moda_baños_por_tipo = df.groupby(['tipo']).agg(lambda x:x.value_counts().index[0]).baños.reset_index(name='moda_baños')\n",
        "moda_baños_por_tipo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze_3sNPjQLO5"
      },
      "source": [
        "###### 2.2. Faltantes de variable `sup_total` y `sup_cubierta`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YJlFzWk9eYR"
      },
      "source": [
        "# Separo los datos que tienen missings de superficie (total o cubierta)\n",
        "mask = np.logical_or(df.sup_cubierta.isna(), df.sup_total.isna())\n",
        "faltantes_sup = df[mask]\n",
        "\n",
        "def sup_faltante(row):\n",
        "  if np.logical_and(pd.isna(row.sup_total), pd.isna(row.sup_cubierta)):\n",
        "    return 'ambas'\n",
        "  elif np.logical_and(pd.isna(row.sup_total), ~pd.isna(row.sup_cubierta)):\n",
        "    return 'total'\n",
        "  elif np.logical_and(~pd.isna(row.sup_total), pd.isna(row.sup_cubierta)):\n",
        "    return 'cubierta'\n",
        "  else:\n",
        "    return 'ninguna'\n",
        "\n",
        "faltantes_sup['falta sup'] = faltantes_sup.apply(sup_faltante, axis=1)\n",
        "faltantes_sup.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt4VdikcJRHU"
      },
      "source": [
        "# Vemos las instancias en las que faltan datos de superficie cubierta y/o de superficie total\n",
        "faltantes_sup.groupby('falta sup').count().precio.reset_index(name='missings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAqXgKVLsS_"
      },
      "source": [
        "sns.countplot(data=faltantes_sup, x='falta sup')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OftzjCz7PXA6"
      },
      "source": [
        "Para los casos en que sólo falta una de las superficies, imputaría el valor de la faltante con el valor de la otra.\n",
        "Para el caso en que faltan ambas superficies las imputaría calculando el precio por metro cuadrado promedio de cada barrio y luego calculo la superficie total dividiendo el `precio` por el `precio_m2` calculado por `barrio`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvN43hm1RJv2"
      },
      "source": [
        "###### 2.3. Faltantes de variables `lat` y `lon`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TNs2SB9RUDA"
      },
      "source": [
        "# Separo los datos que tienen missings de superficie (total o cubierta)\n",
        "mask = np.logical_or(df.lat.isna(), df.lon.isna())\n",
        "faltantes_coord = df[mask]\n",
        "\n",
        "def coord_faltante(row):\n",
        "  if np.logical_and(pd.isna(row.lat), pd.isna(row.lon)):\n",
        "    return 'ambas'\n",
        "  elif np.logical_and(pd.isna(row.lat), ~pd.isna(row.lon)):\n",
        "    return 'lat'\n",
        "  elif np.logical_and(~pd.isna(row.lat), pd.isna(row.lon)):\n",
        "    return 'lon'\n",
        "  else:\n",
        "    return 'ninguna'\n",
        "\n",
        "faltantes_coord['falta coordenada'] = faltantes_coord.apply(coord_faltante, axis=1)\n",
        "faltantes_coord.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFFmQRUGRUDC"
      },
      "source": [
        "faltantes_coord.groupby('falta coordenada').count().precio.reset_index(name='missings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL5GvzfhRUDC"
      },
      "source": [
        "sns.countplot(data=faltantes_coord, x='falta coordenada')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_OMjPdaSLMX"
      },
      "source": [
        "En este caso completaría las coordenadas `lat` y `lon` de modo de imputarles el promedio de los valores correspondientes a los `barrios` a los cuales pertenecen las instancias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbhBwnndSopx"
      },
      "source": [
        "df_con_coord = df[~mask]\n",
        "barrios = df_con_coord.groupby(by='barrio').mean()[['lat', 'lon']].reset_index()\n",
        "barrios"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i5MB8xK0t45"
      },
      "source": [
        "##### 3. Detección y Eliminación de Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ZYAIIApSvz"
      },
      "source": [
        "#### **Implementación de transformaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9BlR4PTl8IE"
      },
      "source": [
        "##### 1. Missings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xiZrUu8nzkn"
      },
      "source": [
        "# Creamos un diccionario para imputar los missings\n",
        "dict_moda_baños_por_tipo = moda_baños_por_tipo.set_index('tipo').transpose().to_dict('records')[0]\n",
        "print(dict_moda_baños_por_tipo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUQRlyV1nM2u"
      },
      "source": [
        "# Imputando missings en `baños`\n",
        "df.baños = df.baños.fillna(df.tipo.map(dict_moda_baños_por_tipo))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hPppWE9rFdW"
      },
      "source": [
        "# Verificamos que no haya más missings en `baños`\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7yYiCqDvQDh"
      },
      "source": [
        "# Imputando missings en `lat` y `lon`\n",
        "dict_media_lat_por_barrio = barrios[['barrio','lat']].set_index('barrio').transpose().to_dict('records')[0]\n",
        "dict_media_lon_por_barrio = barrios[['barrio','lon']].set_index('barrio').transpose().to_dict('records')[0]\n",
        "df.lat = df.lat.fillna(df.barrio.map(dict_media_lat_por_barrio))\n",
        "df.lon = df.lon.fillna(df.barrio.map(dict_media_lon_por_barrio))\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igELGfv0y-xh"
      },
      "source": [
        "# Agregamos una columna auxiliar al dataframe para identificar facilmente si falta\n",
        "# una superficie o las dos, para facilitar la imputación de missings de superficie.\n",
        "df['falta_sup'] = df.apply(sup_faltante, axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chJGUsZa1Vg0"
      },
      "source": [
        "df.falta_sup.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6SnFh209SS"
      },
      "source": [
        "# Imputamos los missings de `sup_total` con los valores de `sup_cubierta`\n",
        "df.loc[df[df.falta_sup == 'total'].index, 'sup_total'] = df[df.falta_sup == 'total'].sup_cubierta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHBEIct41zBm"
      },
      "source": [
        "# Imputamos los missings de `sup_cubierta` con los valores de `sup_total`\n",
        "df.loc[df[df.falta_sup == 'cubierta'].index, 'sup_cubierta'] = df[df.falta_sup == 'cubierta'].sup_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98IokcSk2Ech"
      },
      "source": [
        "# Verificamos que ahora tenemos misma cantidad de missings de `sup_cubierta` y `sup_total`\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812Mw7g58jd2"
      },
      "source": [
        "# Creamos una columna calculada con el precio por metro cuadrado\n",
        "df['precio_m2'] = df.precio / df.sup_total\n",
        "# Y luego creamos un diccionario con el precio por metro cuadrado por barrio\n",
        "dict_precio_m2_por_barrio = df.groupby('barrio').mean()['precio_m2'].to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQi451ciCiVL"
      },
      "source": [
        "# Luego asigno el precio por metro cuadrado de cada barrio a cada fila que contiene faltantes de precio por m2 (debido al faltante de superficie)\n",
        "df.precio_m2 = df.precio_m2.fillna(df.barrio.map(dict_precio_m2_por_barrio))\n",
        "# E imputo los valores faltantes de superficie con el siguiente cálculo: precio dividido el precio por metro cuadrado.\n",
        "df.sup_total.fillna(df.precio / df.precio_m2, inplace=True)\n",
        "# Finalmente imputo la superficie cubierta con el valor de superficie total.\n",
        "df.sup_cubierta.fillna(df.sup_total, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5O_9dplDK67"
      },
      "source": [
        "# Verifico que no tengo más missings.\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70_IjWC_MLq9"
      },
      "source": [
        "# Descarto las columnas auxiliares que no usaremos más.\n",
        "df.drop(columns=['precio_m2', 'falta_sup'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRXqTgz1Qy-E"
      },
      "source": [
        "##### 2. Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_okzYe9QfNC"
      },
      "source": [
        "! pip install feature_engine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv0LnmteQqi0"
      },
      "source": [
        "pip install --upgrade category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IstzkoddQtSw"
      },
      "source": [
        "# Importamos las librerias que son utiles para esto\n",
        "import feature_engine\n",
        "from feature_engine.imputation import AddMissingIndicator, CategoricalImputer, MeanMedianImputer, ArbitraryNumberImputer\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.selection import DropConstantFeatures, DropCorrelatedFeatures, SmartCorrelatedSelection\n",
        "import category_encoders as ce\n",
        "from imblearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMwJl7zYvWz"
      },
      "source": [
        "Posible opción para encoding: asignar un número a cada categoría:\n",
        "\n",
        "Asignar un entero a cada valor de las variables categóricas (tiene la desventaja de que los valores asignados no guardan relación alguna entre ellas, por ejemplo, no se puede inferir por medio de los números asignados si un `barrio` es en promedio más caro que otro, si está más cerca o si las superficies de sus propiedades guardan alguna relación)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkP--3WdVanb"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "df2 = df.copy()\n",
        "cols = ('barrio', 'tipo')\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in cols:\n",
        "    lbl = LabelEncoder() \n",
        "    lbl.fit(list(df2[c].values)) \n",
        "    df2[c] = lbl.transform(list(df[c].values))\n",
        "\n",
        "# shape        \n",
        "print('Shape all_data: {}'.format(df2.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcC6wwjlVoJL"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZoymdnqY6uL"
      },
      "source": [
        "Otra opción es realizar un ***Target Encoding***.\n",
        "\n",
        "Este tipo de encoding permite 'ayudar' al modelo, dar alguna pauta de la relación que existe entre los valores de variables categóricas.\n",
        "Por ejemplo, si hacemos encoding de los `barrios` con relación a la variable `precio`, se podría inferir que a valores más altos de `barrio_code` se corresponden, por lo general, valores más altos de `precio`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luPKyOrxY_D1"
      },
      "source": [
        "df3 = df.copy() # Partimos del dataset orginal\n",
        "cols = ('zona', 'barrio', 'tipo')\n",
        "\n",
        "for col in cols:\n",
        "    lbl=ce.TargetEncoder(cols=col)\n",
        "    df3[col + '_code'] = round(lbl.fit_transform(df3[col], df3['precio']), 0)\n",
        "\n",
        "df3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRcZQhima5WX"
      },
      "source": [
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgUPZaGkds43"
      },
      "source": [
        "# Graficamos el heatmap para monitorear las correlaciones\n",
        "hmap(df3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ6qopLUu-K9"
      },
      "source": [
        "df4 = df3.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efrv6mlBwxNe"
      },
      "source": [
        "###### 1.1. One Hot Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dglpBVST0lPO"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YJzfhBe02Vc"
      },
      "source": [
        "data = df[['barrio', 'tipo', 'sup_total', 'precio']]\n",
        "column_trans = make_column_transformer(\n",
        "    (OneHotEncoder(sparse=False),['barrio', 'tipo']),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "data = data.dropna()\n",
        "X = data[['barrio', 'tipo', 'sup_total']]\n",
        "y = data['precio']\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVz4BvlS2g4U"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rRXwU0m1wEp"
      },
      "source": [
        "column_trans.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ6V1hi5oY9j"
      },
      "source": [
        "##### 3. Ouliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPlga--LoeRW"
      },
      "source": [
        "%unload_ext google.colab.data_table\n",
        "df3.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgTUV-0BNC5V"
      },
      "source": [
        "La superficie mínima (tanto la total como la cubierta) siguen siendo de 1 $m^2$ (valor que no tiene sentido), por lo que voy a filtrar el dataset para superficies mayores a 25 $m^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6H6viZCNCOh"
      },
      "source": [
        "df3 = df3[df3.sup_total > 25]\n",
        "df3 = df3[df3.sup_cubierta > 25]\n",
        "df3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWQyV-JEN4_2"
      },
      "source": [
        "df3.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h64UPIjsvb2"
      },
      "source": [
        "# Vamos a eliminar los outliers con el método del rango intercuartílico.\n",
        "df_out = df3.copy()\n",
        "def removeOutliers(data, col):\n",
        "    Q3 = np.quantile(data[col], 0.75)\n",
        "    Q1 = np.quantile(data[col], 0.25)\n",
        "    IQR = Q3 - Q1\n",
        "      \n",
        "    print(\"El IQR de la columna %s es: %s\" % (col, IQR))\n",
        "    global outlier_free_list\n",
        "    global filtered_data\n",
        "\n",
        "    lower_range = Q1 - 1.5 * IQR\n",
        "    upper_range = Q3 + 1.5 * IQR\n",
        "    outlier_free_list = [x for x in data[col] if (\n",
        "        (x > lower_range) & (x < upper_range))]\n",
        "    filtered_data = data.loc[data[col].isin(outlier_free_list)]\n",
        "\n",
        "columns_to_filter = ['sup_total', 'sup_cubierta', 'precio', 'ambientes', 'dormitorios', 'baños']\n",
        "\n",
        "for i in columns_to_filter:\n",
        "    removeOutliers(df_out, i)\n",
        "    # Assigning filtered data back to our orginal variable\n",
        "    df_out = filtered_data\n",
        "print(\"Shape of data after outlier removal is: \", df_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNlYlCWot39U"
      },
      "source": [
        "def dist_plot(df_in, df_out, col):\n",
        "  plt.figure(figsize=(11,1))\n",
        "  sns.boxplot(data=df_in, x=col)\n",
        "  plt.title('Boxplot del Dataset original')\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(11,1))\n",
        "  sns.boxplot(data=df_out, x=col)\n",
        "  plt.title('Boxplot del Dataset sin outliers')\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(10,4))\n",
        "  g = sns.histplot(data=df_out, x=col, kde=True, bins=50, hue='tipo')\n",
        "  plt.title(\"Histograma del Dataset sin outliers\")\n",
        "  plt.ylabel(\"Cantidad de Propiedades\")\n",
        "  plt.show()\n",
        "\n",
        "dist_plot(df, df_out, 'sup_total')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562ZuSOauVvx"
      },
      "source": [
        "cols = ['sup_total', 'sup_cubierta', 'precio', 'ambientes', 'dormitorios', 'baños']\n",
        "dist_plot(df, df_out, cols[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGNxHTAywyNj"
      },
      "source": [
        "dist_plot(df, df_out, cols[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNZRd6sk3zDS"
      },
      "source": [
        "dist_plot(df, df_out, cols[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am6vxS1e351T"
      },
      "source": [
        "dist_plot(df, df_out, cols[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6pnloO39k2"
      },
      "source": [
        "dist_plot(df, df_out, cols[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYaNOyU44ASw"
      },
      "source": [
        "df_out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OLQfOC9bTpS"
      },
      "source": [
        "# Cantidad de instancias por Tipo de Propiedad\n",
        "tipos = df_out['tipo'].value_counts().to_frame(\"Cantidad\")\n",
        "tipos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIlMmeAjaeAE"
      },
      "source": [
        "# Generamos un gráfico de la distribución de la variable Precio por Tipo de propiedad y las Cantidades de Propiedades por Tipo\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True, gridspec_kw={'width_ratios': [3, 1]})\n",
        "fig.suptitle('Distribución de Precios y Cantidad de Propiedades por Tipo')\n",
        "\n",
        "# Boxplot\n",
        "sns.boxplot(ax=axes[0], x=df_out.precio, y=df_out.tipo, order=tipos.index)\n",
        "axes[0].set_xlabel('Precio [usd]')\n",
        "axes[0].set_ylabel('Tipo de propiedad')\n",
        "\n",
        "# Barplot\n",
        "ax = sns.barplot(ax=axes[1], x=tipos.Cantidad, y=tipos.index, order=tipos.index)\n",
        "axes[1].set_xlabel('Cantidad')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "# label each bar in barplot\n",
        "for p in ax.patches:\n",
        "  height = p.get_height() # height of each horizontal bar is the same\n",
        "  width = max(tipos.values)/2\n",
        "  # adding text to each bar\n",
        "  ax.text(x = width, # x-coordinate position of data label, padded 3 to right of bar\n",
        "    y = p.get_y()+(height/2), # # y-coordinate position of data label, padded to be in the middle of the bar\n",
        "    s = '{:.0f}'.format(p.get_width()),\n",
        "    va = 'center',\n",
        "    fontdict= { 'fontsize': 12})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmUIkVidu8yT"
      },
      "source": [
        "df_out.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJavKvT12wvu"
      },
      "source": [
        "df_out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iKVZxcpa5L"
      },
      "source": [
        "#### **Reentrenamiento de <u>Modelo Sprint 1<u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpdrkPV7WC9M"
      },
      "source": [
        "##### Reentrenamiento de modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSqa1vJQC4PR"
      },
      "source": [
        "# Definimos las variables de predictoras y la variable a predecir\n",
        "data = df_out.copy()\n",
        "X = data.drop(['zona', 'barrio', 'tipo', 'precio'], axis=1)\n",
        "y = data['precio']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1eytqha4oa3"
      },
      "source": [
        "# En caso que la variable predictora sea una sola, graficamos la variable predecir en función de la predictora.\n",
        "# Si no se cumple la condición, no se realiza el gráfico.\n",
        "if X.shape[1] == 1:\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.scatter(X,y, s = 2)\n",
        "  plt.xlabel('Superficie total [m2]')\n",
        "  plt.ylabel('Precio [usd]')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu-zaSrk4s8h"
      },
      "source": [
        "# Hacemos la división entre datos de entrenamiento y datos de testeo.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8245VhvZ44K3"
      },
      "source": [
        "# Importamos las librerías de los modelos a utilizar\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Instanciamos lo modelos\n",
        "linear_model = LinearRegression()\n",
        "tree_regressor = DecisionTreeRegressor(max_depth=11, random_state=0, max_features=2, min_samples_leaf=0.00015704594766823636, min_samples_split= 0.0003043067752731219, criterion='mae')\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsmSNPs48ye"
      },
      "source": [
        "# Entrenamos los modelos\n",
        "linear_model.fit(X_train, y_train)\n",
        "tree_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZxIwmSq5EJL"
      },
      "source": [
        "# En caso que la variable predictora sea una sola, graficamos la variable predecir en función de la predictora para los tres modelos.\n",
        "if X.shape[1] == 1 :\n",
        "  plt.figure(figsize = (20,10))\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.scatter(X,y, s = 2)\n",
        "  plt.plot(X,linear_model.predict(X),label ='Regresion Lineal', c = 'g')\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.scatter(X,y, s = 2)\n",
        "  plt.plot(X,tree_regressor.predict(X),label ='Árbol de Decisión', c = 'g')\n",
        "\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.scatter(X,y, s = 2)\n",
        "  plt.plot(X,knn_regressor.predict(X),label ='knn', c = 'g')\n",
        "\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OXixsLE5G_g"
      },
      "source": [
        "# Luego analizamos la distribución de los errores de las predicciones de cada modelo\n",
        "# y calculamos la raíz del error cuadrático medio de la cada modelo con los hiperparámetros utilizados\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "modelos = ['Regresión lineal', 'Árbol de Decisión', 'Vecinos más cercanos']\n",
        "\n",
        "for i, model in enumerate([linear_model, tree_regressor, knn_regressor]):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    print(f'Modelo: {modelos[i]}')\n",
        "\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
        "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
        "    \n",
        "    plt.figure(figsize = (16,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.histplot(y_train - y_train_pred, bins = 20, label = 'train', kde=True)\n",
        "    sns.histplot(y_test - y_test_pred, bins = 20, label = 'test', kde=True)\n",
        "    plt.xlabel('errores')\n",
        "    plt.xlim(-200000, 200000)\n",
        "    plt.ylim(0, 50000)\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    ax.scatter(y_test,y_test_pred, s =2)\n",
        "    \n",
        "    lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
        "    ]\n",
        "    \n",
        "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "    plt.xlabel('y (test)')\n",
        "    plt.ylabel('y_pred (test)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci1bQwjx9AWA"
      },
      "source": [
        "# Ahora vamos a analizar el R2 que puede darnos una idea más comparable del \n",
        "# ajuste de los datos predichos con los datos reales de la muestra.\n",
        "# Importamos la metrica\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ikTU_B1SgAX"
      },
      "source": [
        "# Primero calculamos el R^2 del benchmark: Modelo de Regresión Lineal\n",
        "r2_test_lin = r2_score(y_test,linear_model.predict(X_test))\n",
        "r2_train_lin = r2_score(y_train,linear_model.predict(X_train))\n",
        "print('r2_train_lin:', r2_train_lin)\n",
        "print('r2_test_lin:', r2_test_lin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjPv1ZBA6f2W"
      },
      "source": [
        "# Vamos a calcular los R2 variando los hiperparámetros de entrenamiento del modelo\n",
        "# Definimos las listas vacias para los valores de accuracy deseados\n",
        "lista_rsme_train = []\n",
        "lista_rsme_test = []\n",
        "lista_r2_train = []\n",
        "lista_r2_test = []\n",
        "\n",
        "# Definimos la lista de valores de k que vamos a explorar\n",
        "max_depths = np.arange(1,25, 1)\n",
        "\n",
        "# Generamos un loop sobre los distintos valores de k \n",
        "for d in max_depths:\n",
        "    \n",
        "    # Vamos a repetir el siguiente bloque de código\n",
        "    \n",
        "    # Definir el modelo con el valor de vecinos deseado\n",
        "    clf = DecisionTreeRegressor(max_depth = d)\n",
        "    \n",
        "    # Entrenar el modelo\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    # train_acc = clf.score(X_train, y_train_pred)\n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    # test_acc = clf.score(X_test, y_test_pred)\n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    lista_rsme_train.append(rmse_train)\n",
        "    lista_rsme_test.append(rmse_test)\n",
        "    lista_r2_train.append(r2_train)\n",
        "    lista_r2_test.append(r2_test)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktyf5eyM99ov"
      },
      "source": [
        "# A continuación creamos el gráfico de los R^2 en función de los niveles de profundidad máxima del modelo de Árboles de decisión.\n",
        "# Adicionalmente, marcamos el punto óptimo que maximiza el R2 en los datos de testeo.\n",
        "# Este punto sería el máximo a considerar para definir la profundidad del Árbol de decisión, ya que queremos minimizar el costo computacional de entrenamiento.\n",
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "\n",
        "x = max_depths\n",
        "y1 = lista_r2_train\n",
        "y2 = lista_r2_test\n",
        "y3 = lista_rsme_train\n",
        "y4 = lista_rsme_test\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(x, y1, 'o-', label='r2 train') # r2 train\n",
        "ax1.plot(x, y2, 'o-', label='r2 test') # r2 test\n",
        "ax2.plot(x, y3, '--', label='rsme train', alpha=1) # rsme train\n",
        "ax2.plot(x, y4, '--', label='rsme test', alpha=1) # rsme test\n",
        "\n",
        "ax1.set_xlabel('Niveles de profundidad')\n",
        "ax1.set_ylabel('$R^2$', rotation=0, size=14, labelpad=20)\n",
        "ax2.set_ylabel('$RMSE$', rotation=0, size=14, labelpad=30)\n",
        "\n",
        "max_r2_test_tree = max(y2)\n",
        "max_depth = x[np.where(y2 == max_r2_test_tree)[0][0]]\n",
        "r_train_tree = y1[np.where(y2 == max_r2_test_tree)[0][0]]\n",
        "\n",
        "ax1.plot(max_depth, max_r2_test_tree, '*r', markersize=20)\n",
        "ax1.plot(max_depth, r_train_tree, '*r', markersize=20)\n",
        "y_min = ax1.get_ylim()[0]\n",
        "y_max = ax1.get_ylim()[1]\n",
        "plt.axvline(x=max_depth, color='r', ymax=(r_train_tree - y_min) / (y_max-y_min), dashes=(4,4))\n",
        "\n",
        "ax1.annotate(np.round(max_r2_test_tree,2), xy=(max_depth, max_r2_test_tree), xytext=(20, -25), size=14, xycoords='data', textcoords='offset points')\n",
        "ax1.annotate(np.round(r_train_tree, 2), xy=(max_depth, r_train_tree), xytext=(20, -20), size=14,xycoords='data', textcoords='offset points')\n",
        "ax1.annotate(\"max_depth = \" + str(np.round(max_depth, 0)), xy=(max_depth, y_min), xytext=(-140, 20), size=14,xycoords='data', textcoords='offset points')\n",
        "\n",
        "plt.title('Ajuste Árboles de decisión')\n",
        "ax1.legend(fontsize=12, loc='best', bbox_to_anchor=(0.5, 0.4, 0.5, 0.5))\n",
        "ax2.legend(fontsize=12, loc='best', bbox_to_anchor=(0.5, -0.2, 0.5, 0.5))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecTPQcjGKgsO"
      },
      "source": [
        "# Repetimos lo anterior, pero para el modelo de Vecinos más cercanos\n",
        "# Definimos las listas vacias para los valores de accuracy deseados\n",
        "lista_rsme_train = []\n",
        "lista_rsme_test = []\n",
        "lista_r2_train = []\n",
        "lista_r2_test = []\n",
        "# time_list = []\n",
        "# fisrt_time = milisecs(datetime.now())\n",
        "\n",
        "# Definimos la lista de valores de k que vamos a explorar\n",
        "k_vecinos = np.arange(1,27, 1)\n",
        "\n",
        "# Generamos un loop sobre los distintos valores de k \n",
        "for k in k_vecinos:\n",
        "    \n",
        "    # Vamos a repetir el siguiente bloque de código\n",
        "    \n",
        "    # Definir el modelo con el valor de vecinos deseado\n",
        "    clf = KNeighborsRegressor(n_neighbors = k)\n",
        "    \n",
        "    # Entrenar el modelo\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    # train_acc = clf.score(X_train, y_train_pred)\n",
        "    # time = milisecs(datetime.now()) - fisrt_time\n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    # test_acc = clf.score(X_test, y_test_pred)\n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    lista_rsme_train.append(rmse_train)\n",
        "    lista_rsme_test.append(rmse_test)\n",
        "    lista_r2_train.append(r2_train)\n",
        "    lista_r2_test.append(r2_test)\n",
        "    # time_list.append(time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5FDSreGEFj"
      },
      "source": [
        "# A continuación creamos el gráfico de los R^2 en función de la cantidad de vecinos del modelo de KNN.\n",
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "\n",
        "x = k_vecinos\n",
        "y1 = lista_r2_train\n",
        "y2 = lista_r2_test\n",
        "y3 = lista_rsme_train\n",
        "y4 = lista_rsme_test\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(x, y1, 'o-', label='r2 train') # r2 train\n",
        "ax1.plot(x, y2, 'o-', label='r2 test') # r2 test\n",
        "ax2.plot(x, y3, '--', label='rsme train', alpha=1) # rsme train\n",
        "ax2.plot(x, y4, '--', label='rsme test', alpha=1) # rsme test\n",
        "\n",
        "ax1.set_xlabel('K vecinos más cercanos')\n",
        "ax1.set_ylabel('$R^2$', rotation=0, size=14, labelpad=20)\n",
        "ax2.set_ylabel('$RMSE$', rotation=0, size=14, labelpad=30)\n",
        "\n",
        "max_r2_test_tree = max(y2)\n",
        "max_depth = x[np.where(y2 == max_r2_test_tree)[0][0]]\n",
        "r_train_tree = y1[np.where(y2 == max_r2_test_tree)[0][0]]\n",
        "\n",
        "ax1.plot(max_depth, max_r2_test_tree, '*r', markersize=20)\n",
        "ax1.plot(max_depth, r_train_tree, '*r', markersize=20)\n",
        "y_min = ax1.get_ylim()[0]\n",
        "y_max = ax1.get_ylim()[1]\n",
        "plt.axvline(x=max_depth, color='r', ymax=(r_train_tree - y_min) / (y_max-y_min), dashes=(4,4))\n",
        "\n",
        "ax1.annotate(np.round(max_r2_test_tree,2), xy=(max_depth, max_r2_test_tree), xytext=(20, -25), size=14, xycoords='data', textcoords='offset points')\n",
        "ax1.annotate(np.round(r_train_tree, 2), xy=(max_depth, r_train_tree), xytext=(20, -20), size=14,xycoords='data', textcoords='offset points')\n",
        "ax1.annotate(\"knn = \" + str(np.round(max_depth, 0)), xy=(max_depth, y_min), xytext=(-80, 20), size=14,xycoords='data', textcoords='offset points')\n",
        "\n",
        "plt.title('Ajuste K vecinos más cercanos')\n",
        "ax1.legend(fontsize=12, loc='lower right')\n",
        "ax2.legend(fontsize=12, loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUSXHocBp881"
      },
      "source": [
        "##### Evaluación de desempeño"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AtwlO2lku9l"
      },
      "source": [
        "# Creamos una tabla con los resultados\n",
        "# LIN = Regresión lineal\n",
        "# TREE = Modelo de Árboles de decisión\n",
        "# LIN = Regresión lineal\n",
        "\n",
        "res2 = pd.DataFrame([['-', np.round(r2_train_lin, 2), np.round(r2_test_lin, 2)],\n",
        "                    [max_depth, np.round(r_train_tree, 2), np.round(max_r2_test_tree, 2)], \n",
        "                    [n, np.round(r_train_knn, 2), np.round(max_r2_test_knn, 2)]], \n",
        "                   columns=['Parámetro', 'R2_train', 'R2_test'], index=['LIN', 'TREE', 'KNN'])\n",
        "res2 = res2.transpose()\n",
        "res2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHKhD_M-nzxn"
      },
      "source": [
        "# Resultados del sprint 1\n",
        "res1 = pd.DataFrame([['-', 0.55, 0.55],[17, 0.95, 0.77],[3, 0.88, 0.75]], \n",
        "                   columns=['Parámetro', 'R2_train', 'R2_test'], index=['LIN', 'TREE', 'KNN'])\n",
        "res1 = res1.transpose()\n",
        "res1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwdwqB_owMg"
      },
      "source": [
        "### **Modelos Avanzados**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTnVOu1tpQF5"
      },
      "source": [
        "#### **Elección, Entrenamiento y Evaluación de Modelos Avanzados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTqpy_u1Q8ZG"
      },
      "source": [
        "##### RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9XxMxz1Jvr8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6k1unuvJvsA"
      },
      "source": [
        "2. Investigar sus parámetros. En particular, `n_estimators`, `max_features` y `oob_score`. Luego, crear y entrenar un modelo en el conjunto de train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKh6si1JvsB"
      },
      "source": [
        "clf = RandomForestRegressor(n_estimators=100, max_features='sqrt', n_jobs=-1, oob_score = True, random_state = 42)\n",
        "clf.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBGH4NQHOdv7"
      },
      "source": [
        "import sklearn.metrics as metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gST5BGbqOElp"
      },
      "source": [
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)\n",
        "print(metrics.mean_squared_error(y_train, y_train_pred, squared=False))\n",
        "print(metrics.mean_squared_error(y_test, y_test_pred, squared=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AbWPlIKJvsJ"
      },
      "source": [
        "clf.oob_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGgD2Xs3JvsL"
      },
      "source": [
        "clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "014iW4IYJvsO"
      },
      "source": [
        "importances = clf.feature_importances_\n",
        "columns = X_train.columns\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize = (15,8))\n",
        "sns.barplot(columns[indices], importances[indices])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyqih71er1Wm"
      },
      "source": [
        "#### **Optimización de hiperparámetros**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxrFEf0iVMVY"
      },
      "source": [
        "##### GPMINIMIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK5F69HBRKst"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP8MYrhkRKsu"
      },
      "source": [
        "# Creamos el objeto regresor\n",
        "regressor = DecisionTreeRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnGpXg3kRKsu"
      },
      "source": [
        "# Lo fitteamos a la data de train\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rj0k49kRKsu"
      },
      "source": [
        "# Definimos aquellos parámetros que queremos colocar, pero que queremos NO mover luego\n",
        "FIXED_PARAMS = {         \n",
        "                'random_state': 0,           \n",
        "                     }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMavN92GRKsu"
      },
      "source": [
        "Ahora definiremos el espacio de búsqueda. Es decir, entre que valores vamos a buscar los hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr2Z_T4wsy4k"
      },
      "source": [
        "! pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdF8nc6BRKsv"
      },
      "source": [
        "from skopt.space import Real, Integer, Categorical\n",
        "cant_columnas = X_train.shape[1]\n",
        "space= [          \n",
        "             Categorical(['mse','mae'], name='criterion')\n",
        "            ,Integer(2, 100, name='max_depth') # maxima profundidad de cada árbol (aquí sí es mejor que sean profundos, porque no se concatenan, son independientes)\n",
        "            ,Integer(10, 100, name='min_samples_split') # mínima cantidad de registros para abrir una rama\n",
        "            ,Integer(5, 50, name='min_samples_leaf') # mínima cantidad de registros para abrir una hoja\n",
        "            ,Integer(round(cant_columnas*0.1), round(cant_columnas*0.8), name='max_features') # máxima cantidad de atributos (columnas) que puede usar cada árbol\n",
        "            ]\n",
        "# listamos los nombres de los parámetros cuyo espacio de búsqueda acabamos de definir\n",
        "param_names = ['criterion','max_depth','min_samples_split','min_samples_leaf','max_features']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm8cwpnwRKsv"
      },
      "source": [
        "Definimos una funcion de métricas de evaluacion (para obtener RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6XzOAm9RKsw"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "def eval_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False) \n",
        "    return rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhtHO1LdRKsw"
      },
      "source": [
        "Lo más importante: definimos todo lo que es nuestra \"función objetivo\". Es decir, lo que queremos que se haga en cada iteración!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERndOkgpRKsw"
      },
      "source": [
        "# Definimos la funcion objetivo, que se utilizará a cada iteración de la búsqueda\n",
        "# creamos una lista para guardar los resultados\n",
        "lista_results = []\n",
        "lista_test_scores_cv = []\n",
        "lista_train_scores_cv = []\n",
        "lista_test_std_cv = []\n",
        "\n",
        "from skopt.utils import use_named_args\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "\n",
        "    # seteamos los parámetros fijos\n",
        "    regressor.set_params(**FIXED_PARAMS)\n",
        "    # y los parámetros de cada iteración (se setearán automáticamente así)\n",
        "    regressor.set_params(**params)\n",
        "    \n",
        "    ####################################################################################\n",
        "    ############################# USAMOS CROSS VALIDATION ##############################\n",
        "    ####################################################################################\n",
        "    # Ahora en vez de eso calculamos nuestros scores de test de un cross validation\n",
        "    # Recordemos, este objeto devuelve un array de todos los test_scores y tambien de los train_scores hallados --> nuestra loss será el promedio de los test_scores\n",
        "    model_i_scores = cross_validate(regressor, X_train, y_train, cv=5, scoring=  make_scorer(eval_metrics), return_train_score=True)\n",
        "    mean_test_scores = model_i_scores['test_score'].mean()\n",
        "    # Peeero tambien nos guardamos los promedios de train para poder ver si en general se estuvo overfiteando o no \n",
        "    mean_train_scores = model_i_scores['train_score'].mean()\n",
        "    # Por último, resulta interesante ver cuan volatil fueron esos test_scores. Así que tomamos su desvio standard (std)\n",
        "    std_test_scores = model_i_scores['test_score'].std()\n",
        "    \n",
        "    # Ahora guardamos estos datos en las listas vacias que creamos antes, así nos quedan bien guardados\n",
        "    lista_test_scores_cv.append(mean_test_scores) # test\n",
        "    lista_train_scores_cv.append(mean_train_scores) # train\n",
        "    lista_test_std_cv.append(std_test_scores) # std de test\n",
        " \n",
        "    # Definimos que la funcion de perdida sea el promedio de los scores de test\n",
        "    loss = mean_test_scores\n",
        "    ####################################################################################\n",
        "    ####################################################################################\n",
        "    ####################################################################################\n",
        "    \n",
        "    return loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlLkfR79RKsy"
      },
      "source": [
        "# Ejecutamos la búsqueda de los hiperparámetros \n",
        "\n",
        "from skopt import gp_minimize, forest_minimize, dump\n",
        "# import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Armamos una clase con tqdm para poder ver el progreso de la búsqueda\n",
        "class tqdm_skopt(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        self._bar = tqdm(**kwargs)\n",
        "    def __call__(self, res):\n",
        "        self._bar.update()\n",
        "\n",
        "# Cantidad de iteraciones para la búsqueda (utilizar muchas, aqui usamos 50 solo por probar)\n",
        "cantidad_iteraciones = 200\n",
        "\n",
        "# Búsqueda\n",
        "res = gp_minimize(\n",
        "                    objective\n",
        "                    ,space\n",
        "                    ,n_calls = cantidad_iteraciones\n",
        "                    ,n_initial_points = int(round(cantidad_iteraciones*0.2)) # Cantidad de iteraciones iniciales random (20% es significativo, pero dando mucho espacio para que el algoritmo optimice)\n",
        "                    ,n_jobs=1\n",
        "                    ,random_state = 0\n",
        "                    ,verbose=1\n",
        "                    ,callback=[tqdm_skopt(total=cantidad_iteraciones, desc=\"Gaussian Process\")]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQe23GJVRKsy",
        "scrolled": true
      },
      "source": [
        "res.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxm_vHKyRKsz"
      },
      "source": [
        "res.fun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGlVt3oPRKsz"
      },
      "source": [
        "res.space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTjrBlqKRKs0"
      },
      "source": [
        "res.specs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlS_WV9iRKs0"
      },
      "source": [
        "res.x_iters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0hRhCNRKs0"
      },
      "source": [
        "res.func_vals "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaJEEoJWRKs1"
      },
      "source": [
        "Armamos un dataframe con los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksGHJZ4VRKs1"
      },
      "source": [
        "df_vemos_que_paso = pd.DataFrame(res.x_iters)\n",
        "df_vemos_que_paso.columns = param_names\n",
        "df_vemos_que_paso['funcion_costo'] = res.func_vals\n",
        "df_vemos_que_paso['numero_de_iteracion'] = df_vemos_que_paso.reset_index()['index']\n",
        "df_vemos_que_paso['score_train'] = lista_train_scores_cv[:200]\n",
        "df_vemos_que_paso['score_test'] = lista_test_scores_cv[:200]\n",
        "df_vemos_que_paso['dif_train_test'] = df_vemos_que_paso['score_test'] - df_vemos_que_paso['score_train']\n",
        "df_vemos_que_paso['std_test'] = lista_test_std_cv[:200]\n",
        "df_vemos_que_paso.sort_values('funcion_costo', ascending=True, inplace=True)\n",
        "df_vemos_que_paso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUR5L02mRKs1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(df_vemos_que_paso['numero_de_iteracion'], df_vemos_que_paso['funcion_costo'], '--')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PY-eojJRKs1"
      },
      "source": [
        "Armamos un diccionario con los mejores hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIAw3KX2RKs2"
      },
      "source": [
        "# Armamos un diccionario a partir de 2 listas: la primera es los nombres que tienen los hiperparametros, y la otra son los mejores que se encontraron\n",
        "mejores_hp = dict(zip(param_names, res.x))\n",
        "# Pero adem{as le agregamos aquellos hiperparametros que dejamos fijos (en nuestro caso solo fue el random_state)\n",
        "mejores_hp.update(FIXED_PARAMS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61_2CjuQRKs2"
      },
      "source": [
        "mejores_hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6t3xOPRKs2"
      },
      "source": [
        "# Colocamos esos mejores hiperparametros en nuestro modelo\n",
        "regressor.set_params(**mejores_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT5nrBknRKs2"
      },
      "source": [
        "# Entrenamos ese arbol que ya tiene los hiperparametros correctos\n",
        "regressor.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCvFE5XMjZ5B"
      },
      "source": [
        "pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-IPMMPFRKs3"
      },
      "source": [
        "import shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKx5pEhSRKs3"
      },
      "source": [
        "# Utilizamos el explicador de SHAP\n",
        "explainer = shap.TreeExplainer(regressor)\n",
        "# Y pedimos que nos otorgue los valores de importancia que tendrá en el dataframe de validacion\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_G2RqkTRKs3"
      },
      "source": [
        "# Graficamos solo la importancia de cada variable\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLjKkAtqRKs3"
      },
      "source": [
        "# Graficamos el impacto de cada variable \n",
        "# recordemos, para leer el grafico: \n",
        "# - los colores significan los valores de la variable --> ej: un punto de V5 que esté en color fucsia, es un valor alto de V5\n",
        "# - el eje x significa cuanto impacta sobre el output --> ej: los puntos fucsias de V5 están a la derecha, así que impactan positivamente sobre el output\n",
        "# Entonces: A mayor valor de V5, mayor valor del output en general. Y así se podría analizar cada variable\n",
        "\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ofJwqiRKs4"
      },
      "source": [
        "# Graficamos como queda el arbol terminado\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,20))\n",
        "tree.plot_tree(regressor) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAY0NU6BVHlR"
      },
      "source": [
        "##### OPTUNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzgNsDsnRKs5"
      },
      "source": [
        "! pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJaqu44SRKs5"
      },
      "source": [
        "param_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdLGSE6lRKs6"
      },
      "source": [
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0gedZhmRKs6"
      },
      "source": [
        "def objective(trial):    \n",
        "    \n",
        "    joblib.dump(study, 'study.pkl')\n",
        "    \n",
        "    tree_criterion = trial.suggest_categorical('criterion', ['mse', 'mae']) \n",
        "    tree_max_depth = trial.suggest_int('max_depth', 2, 200) \n",
        "    tree_min_samples_split = trial.suggest_int('min_samples_split', 10, 100) \n",
        "    tree_min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 50) \n",
        "    tree_max_features = trial.suggest_int('max_features', round(cant_columnas*0.1), round(cant_columnas*0.8)) \n",
        "\n",
        "    params = {\n",
        "        'criterion': tree_criterion,\n",
        "        'max_depth': tree_max_depth,\n",
        "        'min_samples_split': tree_min_samples_split,\n",
        "        'min_samples_leaf': tree_min_samples_leaf,\n",
        "        'max_features': tree_max_features\n",
        "    }\n",
        "    \n",
        "    regressor.set_params(**params)\n",
        "\n",
        "    ####################################################################################\n",
        "    ############################# USAMOS CROSS VALIDATION ##############################\n",
        "    ####################################################################################\n",
        "    # Ahora en vez de eso calculamos nuestros scores de test de un cross validation\n",
        "    # Recordemos, este objeto devuelve un array de todos los test_scores y tambien de los train_scores hallados --> nuestra loss será el promedio de los test_scores\n",
        "    model_i_scores = cross_validate(regressor, X_train, y_train, cv=3, scoring=  make_scorer(eval_metrics), return_train_score=True)\n",
        "    mean_test_scores = model_i_scores['test_score'].mean()\n",
        "    # Peeero tambien nos guardamos los promedios de train para poder ver si en general se estuvo overfitteando o no \n",
        "    mean_train_scores = model_i_scores['train_score'].mean()\n",
        "    # Por último, resulta interesante ver cuan volatil fueron esos test_scores. Así que tomamos su desvio standard (std)\n",
        "    std_test_scores = model_i_scores['test_score'].std()\n",
        "    \n",
        "    # Ahora guardamos estos datos en las listas vacias que creamos antes, así nos quedan bien guardados\n",
        "    lista_test_scores_cv.append(mean_test_scores) # test\n",
        "    lista_train_scores_cv.append(mean_train_scores) # train\n",
        "    lista_test_std_cv.append(std_test_scores) # std de test\n",
        " \n",
        "    # Definimos que la funcion de perdida sea el promedio de los scores de test\n",
        "    loss = mean_test_scores\n",
        "    ####################################################################################\n",
        "    ####################################################################################\n",
        "    ####################################################################################\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptvN-_zbRKs6"
      },
      "source": [
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "720GtJFVRKs7"
      },
      "source": [
        "study = optuna.create_study()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WjCykiARKs7"
      },
      "source": [
        "study.optimize(objective, n_trials=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSWZtaVtRKs7"
      },
      "source": [
        "study.best_trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCRV0Q3FRKs8"
      },
      "source": [
        "study.best_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkUHMo0uTdx-"
      },
      "source": [
        "#### **Comparación de Modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atnTsOKpr-Io"
      },
      "source": [
        "##### **Comparación de los tres modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQC5quvKRKs8"
      },
      "source": [
        "###### A quien le fue mejor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XyoQckXRKs8"
      },
      "source": [
        "print(f\"El RMSE al que llegó GP_MINIMIZE fue {res.fun}\")\n",
        "print(f\"El RMSE al que llegó OPTUNA fue {study.best_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTjRn_SURKs9"
      },
      "source": [
        "if float(res.fun) > float(study.best_value):\n",
        "    print(\"El ganador fue OPTUNA porque llegó al menor valor\")\n",
        "    \n",
        "elif float(res.fun) < float(study.best_value):\n",
        "    print(\"El ganador fue GP_MINIMIZE porque llegó al menor valor\")\n",
        "    \n",
        "elif float(res.fun) == float(study.best_value):\n",
        "    print(\"Empate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjzXf2J7owPi"
      },
      "source": [
        "### **Interpretación de Modelos**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Uab1xrpQcW"
      },
      "source": [
        "#### **Responder preguntas planteadas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zIyNyKPsaxY"
      },
      "source": [
        "#### **Distribución de los errores**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqKLm9n5sd8h"
      },
      "source": [
        "#### **Fallas de los modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxHW3Gj8owSf"
      },
      "source": [
        "### **Cierre**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiaVWMDBpQxK"
      },
      "source": [
        "#### **Conclusiones**"
      ]
    }
  ]
}